#!/usr/bin/env python3
"""
Comprehensive test suite for PipelineComposer multi-stage workflow orchestration.

Tests the complete implementation of PipelineComposer including:
- Pipeline registration and dependency management
- Sequential and parallel execution strategies
- Dependency resolution and cycle detection
- Metadata enrichment and error handling
- Integration with DataSciencePipeline
"""

import time
import pytest
import pandas as pd
import numpy as np
from unittest.mock import Mock, patch
from typing import Dict, Any

# Import the classes we're testing
from src.localdata_mcp.pipeline.core import DataSciencePipeline, PipelineComposer
from src.localdata_mcp.pipeline.base import PipelineResult, CompositionMetadata, ErrorClassification, PipelineError

# Mock scikit-learn components for testing
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

# Create mock pipeline components for testing
class MockScaler(BaseEstimator, TransformerMixin):
    """Mock scaler for testing."""
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        # Simple mock transformation - just return the data
        return X

class MockModel(BaseEstimator):
    """Mock model for testing."""
    
    def fit(self, X, y=None):
        return self
    
    def predict(self, X):
        # Return simple predictions
        return np.ones(len(X))

class MockAnalyzer(BaseEstimator, TransformerMixin):
    """Mock analyzer that returns statistics."""
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        # Return summary statistics as a DataFrame
        return pd.DataFrame({
            'mean': X.mean().values,
            'std': X.std().values,
            'count': [len(X)] * len(X.columns)
        })

class TestPipelineComposer:
    """Test suite for PipelineComposer orchestration capabilities."""
    
    @pytest.fixture
    def sample_data(self):
        """Create sample DataFrame for testing."""
        np.random.seed(42)
        return pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(5, 2, 100),
            'feature3': np.random.uniform(0, 10, 100)
        })
    
    @pytest.fixture
    def mock_pipelines(self):
        """Create mock DataSciencePipeline instances for testing."""
        # Data cleaning pipeline
        cleaning_pipeline = DataSciencePipeline([
            ('scaler', MockScaler())
        ], analytical_intention="Clean and scale data")
        
        # Statistical analysis pipeline
        stats_pipeline = DataSciencePipeline([
            ('analyzer', MockAnalyzer())
        ], analytical_intention="Generate statistical summaries")
        
        # ML pipeline
        ml_pipeline = DataSciencePipeline([
            ('scaler', MockScaler()),
            ('model', MockModel())
        ], analytical_intention="Train machine learning model")
        
        return {
            'cleaning': cleaning_pipeline,
            'stats': stats_pipeline, 
            'ml': ml_pipeline
        }
    
    def test_composer_initialization(self):
        """Test PipelineComposer initialization with various configurations."""
        # Default initialization
        composer = PipelineComposer()
        assert composer.composition_strategy == 'sequential'
        assert composer.metadata_enrichment == True
        assert composer.streaming_aware == True
        assert composer.error_recovery_mode == 'partial'
        assert composer.max_parallel_pipelines == 4
        assert len(composer.registered_pipelines) == 0
        assert isinstance(composer.composer_id, str)
        
        # Custom initialization
        composer = PipelineComposer(
            composition_strategy='parallel',
            metadata_enrichment=False,
            streaming_aware=False,
            error_recovery_mode='strict',
            max_parallel_pipelines=8,
            composition_timeout_seconds=1800
        )
        assert composer.composition_strategy == 'parallel'
        assert composer.metadata_enrichment == False
        assert composer.streaming_aware == False
        assert composer.error_recovery_mode == 'strict'
        assert composer.max_parallel_pipelines == 8
        assert composer.composition_timeout_seconds == 1800
    
    def test_add_pipeline_basic(self, mock_pipelines):
        """Test basic pipeline registration without dependencies."""
        composer = PipelineComposer()
        
        # Add single pipeline
        composer.add_pipeline('cleaning', mock_pipelines['cleaning'])
        
        assert len(composer.registered_pipelines) == 1
        assert 'cleaning' in composer.registered_pipelines
        assert composer.registered_pipelines['cleaning'] == 'DataSciencePipeline'
        
        # Add multiple pipelines
        composer.add_pipeline('stats', mock_pipelines['stats'])
        composer.add_pipeline('ml', mock_pipelines['ml'])
        
        assert len(composer.registered_pipelines) == 3
        assert set(composer.registered_pipelines.keys()) == {'cleaning', 'stats', 'ml'}
    
    def test_add_pipeline_with_dependencies(self, mock_pipelines):
        """Test pipeline registration with dependencies."""
        composer = PipelineComposer()
        
        # Add base pipeline
        composer.add_pipeline('cleaning', mock_pipelines['cleaning'])
        
        # Add dependent pipeline
        composer.add_pipeline('stats', mock_pipelines['stats'], depends_on='cleaning')
        
        # Check dependency graph
        dependency_report = composer.resolve_dependencies()
        assert dependency_report['execution_order'] == ['cleaning', 'stats']
        assert dependency_report['dependency_graph']['stats'] == ['cleaning']
        assert dependency_report['reverse_dependencies']['cleaning'] == ['stats']
    
    def test_add_pipeline_multiple_dependencies(self, mock_pipelines):
        """Test pipeline with multiple dependencies."""
        composer = PipelineComposer()
        
        # Add base pipelines
        composer.add_pipeline('cleaning', mock_pipelines['cleaning'])
        composer.add_pipeline('stats', mock_pipelines['stats'])
        
        # Add pipeline dependent on multiple others
        composer.add_pipeline('ml', mock_pipelines['ml'], depends_on=['cleaning', 'stats'])
        
        dependency_report = composer.resolve_dependencies()
        assert set(dependency_report['dependency_graph']['ml']) == {'cleaning', 'stats'}
        assert 'ml' in dependency_report['reverse_dependencies']['cleaning']
        assert 'ml' in dependency_report['reverse_dependencies']['stats']
    
    def test_add_pipeline_error_cases(self, mock_pipelines):
        """Test error handling during pipeline registration."""
        composer = PipelineComposer()
        
        # Test duplicate pipeline name
        composer.add_pipeline('test', mock_pipelines['cleaning'])
        with pytest.raises(ValueError, match=\"Pipeline 'test' already exists\"):
            composer.add_pipeline('test', mock_pipelines['stats'])
        
        # Test invalid dependency
        with pytest.raises(ValueError, match=\"Dependency 'nonexistent' not found\"):
            composer.add_pipeline('invalid', mock_pipelines['ml'], depends_on='nonexistent')
    
    def test_dependency_resolution_simple(self, mock_pipelines):
        """Test dependency resolution for simple linear dependencies."""
        composer = PipelineComposer()
        
        # Create linear dependency chain: cleaning -> stats -> ml
        composer.add_pipeline('ml', mock_pipelines['ml'])
        composer.add_pipeline('cleaning', mock_pipelines['cleaning'])
        composer.add_pipeline('stats', mock_pipelines['stats'], depends_on='cleaning')
        composer.add_pipeline('final_ml', mock_pipelines['ml'], depends_on='stats')
        
        dependency_report = composer.resolve_dependencies()
        
        # Check execution order respects dependencies
        execution_order = dependency_report['execution_order']
        assert execution_order.index('cleaning') < execution_order.index('stats')
        assert execution_order.index('stats') < execution_order.index('final_ml')
        
        # ml should be independent and can be first or last
        assert 'ml' in execution_order
        
        # Should have valid parallel groups
        assert len(dependency_report['parallel_groups']) > 0
        assert dependency_report['total_pipelines'] == 4
    
    def test_dependency_resolution_parallel_groups(self, mock_pipelines):
        """Test identification of parallel execution groups."""
        composer = PipelineComposer()
        
        # Create scenario with parallelizable pipelines
        composer.add_pipeline('cleaning', mock_pipelines['cleaning'])
        composer.add_pipeline('stats1', mock_pipelines['stats'], depends_on='cleaning')
        composer.add_pipeline('stats2', mock_pipelines['stats'], depends_on='cleaning')
        composer.add_pipeline('ml1', mock_pipelines['ml'], depends_on='cleaning')
        composer.add_pipeline('ml2', mock_pipelines['ml'], depends_on='cleaning')
        
        dependency_report = composer.resolve_dependencies()
        
        # stats1, stats2, ml1, ml2 should be parallelizable since they all depend only on cleaning
        parallel_groups = dependency_report['parallel_groups']\n        parallelizable_count = sum(len(group) for group in parallel_groups if len(group) > 1)\n        assert parallelizable_count > 1\n        assert dependency_report['parallelizable_pipelines'] > 1\n    \n    def test_circular_dependency_detection(self, mock_pipelines):\n        \"\"\"Test detection of circular dependencies.\"\"\"\n        composer = PipelineComposer()\n        \n        # Create circular dependency: A -> B -> C -> A\n        composer.add_pipeline('A', mock_pipelines['cleaning'])\n        composer.add_pipeline('B', mock_pipelines['stats'], depends_on='A')\n        composer.add_pipeline('C', mock_pipelines['ml'], depends_on='B')\n        \n        # This should work so far\n        composer.resolve_dependencies()\n        \n        # Now create the circular dependency by making A depend on C\n        # We need to manipulate internal state since add_pipeline prevents this\n        composer._dependency_graph['A'] = ['C']\n        composer._reverse_dependencies['C'].append('A')\n        \n        # This should raise an error\n        with pytest.raises(ValueError, match=\"Circular dependency detected\"):\n            composer.resolve_dependencies()\n    \n    def test_sequential_execution(self, sample_data, mock_pipelines):\n        \"\"\"Test sequential pipeline execution with data flow.\"\"\"\n        composer = PipelineComposer('sequential')\n        \n        # Create sequential workflow\n        composer.add_pipeline('cleaning', mock_pipelines['cleaning'])\n        composer.add_pipeline('stats', mock_pipelines['stats'], depends_on='cleaning')\n        \n        # Execute composition\n        results = composer.execute(sample_data)\n        \n        # Check results\n        assert len(results) == 2\n        assert 'cleaning' in results\n        assert 'stats' in results\n        \n        # All results should be successful\n        assert all(result.success for result in results.values())\n        \n        # Check execution metadata\n        for pipeline_name, result in results.items():\n            assert result.execution_time_seconds > 0\n            assert isinstance(result.data, (pd.DataFrame, np.ndarray))\n            assert result.pipeline_stage in ['completed', 'transform']\n    \n    def test_parallel_execution(self, sample_data, mock_pipelines):\n        \"\"\"Test parallel pipeline execution.\"\"\"\n        composer = PipelineComposer('parallel')\n        \n        # Create independent pipelines for parallel execution\n        composer.add_pipeline('stats1', mock_pipelines['stats'])\n        composer.add_pipeline('stats2', mock_pipelines['stats']) \n        composer.add_pipeline('ml', mock_pipelines['ml'])\n        \n        # Execute composition\n        start_time = time.time()\n        results = composer.execute(sample_data)\n        execution_time = time.time() - start_time\n        \n        # Check results\n        assert len(results) == 3\n        assert all(pipeline_name in results for pipeline_name in ['stats1', 'stats2', 'ml'])\n        assert all(result.success for result in results.values())\n        \n        # Parallel execution should be reasonably fast\n        # (This is a rough check - in practice with real pipelines it would be more significant)\n        assert execution_time < 10  # Should complete quickly for mock pipelines\n    \n    def test_adaptive_execution_strategy(self, sample_data, mock_pipelines):\n        \"\"\"Test adaptive execution strategy selection.\"\"\"\n        composer = PipelineComposer('adaptive')\n        \n        # Test with small data and no dependencies (should choose parallel)\n        composer.add_pipeline('stats1', mock_pipelines['stats'])\n        composer.add_pipeline('stats2', mock_pipelines['stats'])\n        composer.add_pipeline('ml', mock_pipelines['ml'])\n        \n        # Execute and check that it completed successfully\n        results = composer.execute(sample_data)\n        assert len(results) == 3\n        assert all(result.success for result in results.values())\n        \n        # Test with dependencies (should choose sequential)\n        composer_seq = PipelineComposer('adaptive')\n        composer_seq.add_pipeline('cleaning', mock_pipelines['cleaning'])\n        composer_seq.add_pipeline('stats', mock_pipelines['stats'], depends_on='cleaning')\n        \n        results_seq = composer_seq.execute(sample_data)\n        assert len(results_seq) == 2\n        assert all(result.success for result in results_seq.values())\n    \n    def test_composition_metadata_enrichment(self, sample_data, mock_pipelines):\n        \"\"\"Test composition metadata generation and enrichment.\"\"\"\n        composer = PipelineComposer('sequential', metadata_enrichment=True)\n        \n        composer.add_pipeline('cleaning', mock_pipelines['cleaning'])\n        composer.add_pipeline('stats', mock_pipelines['stats'], depends_on='cleaning')\n        \n        # Execute to generate metadata\n        results = composer.execute(sample_data)\n        \n        # Check composition metadata\n        metadata = composer.composition_metadata\n        assert metadata is not None\n        assert isinstance(metadata, CompositionMetadata)\n        \n        # Check metadata fields\n        assert metadata.domain in ['general_ml', 'multi_domain', 'ml']\n        assert metadata.analysis_type in ['multi_stage_analysis', 'analysis']\n        assert metadata.result_type == 'multi_pipeline_composition'\n        assert len(metadata.compatible_tools) > 0\n        assert len(metadata.suggested_compositions) > 0\n        assert metadata.confidence_level > 0\n        assert metadata.quality_score > 0\n        \n        # Check data artifacts\n        artifacts = metadata.data_artifacts\n        assert artifacts['total_pipelines'] == 2\n        assert artifacts['execution_strategy'] == 'sequential'\n        assert 'successful_pipelines' in artifacts\n        assert 'total_execution_time' in artifacts\n    \n    def test_error_recovery_modes(self, sample_data, mock_pipelines):\n        \"\"\"Test different error recovery modes.\"\"\"\n        # Create a pipeline that will fail\n        failing_pipeline = DataSciencePipeline([\n            ('failer', Mock(side_effect=Exception(\"Test error\")))\n        ], analytical_intention=\"This will fail\")\n        \n        # Test partial recovery mode (default)\n        composer_partial = PipelineComposer('sequential', error_recovery_mode='partial')\n        composer_partial.add_pipeline('good', mock_pipelines['stats'])\n        composer_partial.add_pipeline('bad', failing_pipeline, depends_on='good')\n        \n        # Should return partial results\n        results_partial = composer_partial.execute(sample_data)\n        assert len(results_partial) >= 1  # At least the successful one\n        assert 'good' in results_partial\n        assert results_partial['good'].success\n        \n        # Test strict mode\n        composer_strict = PipelineComposer('sequential', error_recovery_mode='strict')\n        composer_strict.add_pipeline('good', mock_pipelines['stats'])\n        composer_strict.add_pipeline('bad', failing_pipeline, depends_on='good')\n        \n        # Should raise exception in strict mode\n        with pytest.raises(PipelineError):\n            composer_strict.execute(sample_data)\n    \n    def test_data_transformation_between_pipelines(self, sample_data, mock_pipelines):\n        \"\"\"Test data transformation functions between pipeline stages.\"\"\"\n        composer = PipelineComposer('sequential')\n        \n        # Add transformation function\n        def add_column(data):\n            \"\"\"Add a new column to the data.\"\"\"\n            result = data.copy()\n            result['transformed'] = result['feature1'] * 2\n            return result\n        \n        composer.add_pipeline('first', mock_pipelines['cleaning'])\n        composer.add_pipeline('second', mock_pipelines['stats'], \n                            depends_on='first',\n                            data_transformation=add_column)\n        \n        # Execute and verify transformation was applied\n        results = composer.execute(sample_data)\n        \n        assert len(results) == 2\n        assert all(result.success for result in results.values())\n        \n        # The transformation should have been applied between stages\n        # (We can't directly verify this with mock pipelines, but the execution should succeed)\n    \n    def test_composition_summary(self, sample_data, mock_pipelines):\n        \"\"\"Test composition summary generation.\"\"\"\n        composer = PipelineComposer('sequential')\n        \n        composer.add_pipeline('cleaning', mock_pipelines['cleaning'])\n        composer.add_pipeline('stats', mock_pipelines['stats'], depends_on='cleaning')\n        \n        # Get summary before execution\n        summary_before = composer.get_composition_summary()\n        assert summary_before['total_pipelines'] == 2\n        assert summary_before['composition_strategy'] == 'sequential'\n        assert len(summary_before['registered_pipelines']) == 2\n        assert 'execution_results' not in summary_before\n        \n        # Execute and get summary after\n        results = composer.execute(sample_data)\n        summary_after = composer.get_composition_summary()\n        \n        assert 'execution_results' in summary_after\n        exec_results = summary_after['execution_results']\n        assert len(exec_results['successful_pipelines']) == 2\n        assert len(exec_results['failed_pipelines']) == 0\n        assert exec_results['total_execution_time'] > 0\n        assert exec_results['total_memory_used'] >= 0\n    \n    def test_pipeline_memory_tracking(self, sample_data, mock_pipelines):\n        \"\"\"Test memory usage tracking during execution.\"\"\"\n        composer = PipelineComposer('sequential')\n        \n        composer.add_pipeline('stats', mock_pipelines['stats'])\n        \n        results = composer.execute(sample_data)\n        \n        # Check that memory usage is tracked\n        assert 'stats' in results\n        result = results['stats']\n        assert result.memory_used_mb >= 0  # Should be non-negative\n    \n    def test_streaming_awareness(self, mock_pipelines):\n        \"\"\"Test streaming awareness for large datasets.\"\"\"\n        # Create large dataset that should trigger streaming\n        large_data = pd.DataFrame({\n            'feature1': np.random.normal(0, 1, 10000),\n            'feature2': np.random.normal(5, 2, 10000),\n        })\n        \n        composer = PipelineComposer('sequential', streaming_aware=True)\n        composer.add_pipeline('stats', mock_pipelines['stats'])\n        \n        # Should execute successfully even with large data\n        results = composer.execute(large_data)\n        assert len(results) == 1\n        assert results['stats'].success\n    \n    def test_timeout_handling(self, sample_data, mock_pipelines):\n        \"\"\"Test timeout handling in parallel execution.\"\"\"\n        # Create a composer with very short timeout\n        composer = PipelineComposer('parallel', composition_timeout_seconds=0.001)\n        \n        composer.add_pipeline('stats1', mock_pipelines['stats'])\n        composer.add_pipeline('stats2', mock_pipelines['stats'])\n        \n        # Execution might timeout, but should handle it gracefully\n        # (The timeout is very short, so this tests the timeout mechanism)\n        try:\n            results = composer.execute(sample_data)\n            # If it doesn't timeout, that's also fine - just check results are reasonable\n            assert isinstance(results, dict)\n        except Exception as e:\n            # Timeout exceptions should be handled gracefully\n            assert 'timeout' in str(e).lower() or isinstance(e, concurrent.futures.TimeoutError)\n\n\nif __name__ == '__main__':\n    \"\"\"Run the test suite.\"\"\"\n    print(\"Running PipelineComposer Test Suite...\")\n    \n    # Create test instance\n    test_instance = TestPipelineComposer()\n    \n    # Generate test data\n    np.random.seed(42)\n    sample_data = pd.DataFrame({\n        'feature1': np.random.normal(0, 1, 100),\n        'feature2': np.random.normal(5, 2, 100),\n        'feature3': np.random.uniform(0, 10, 100)\n    })\n    \n    # Create mock pipelines\n    mock_pipelines = {\n        'cleaning': DataSciencePipeline([\n            ('scaler', MockScaler())\n        ], analytical_intention=\"Clean and scale data\"),\n        'stats': DataSciencePipeline([\n            ('analyzer', MockAnalyzer())\n        ], analytical_intention=\"Generate statistical summaries\"),\n        'ml': DataSciencePipeline([\n            ('scaler', MockScaler()),\n            ('model', MockModel())\n        ], analytical_intention=\"Train machine learning model\")\n    }\n    \n    try:\n        # Run key tests manually\n        print(\"\\n1. Testing Composer Initialization...\")\n        test_instance.test_composer_initialization()\n        print(\"‚úì Composer initialization tests passed\")\n        \n        print(\"\\n2. Testing Pipeline Registration...\")\n        test_instance.test_add_pipeline_basic(mock_pipelines)\n        print(\"‚úì Pipeline registration tests passed\")\n        \n        print(\"\\n3. Testing Dependency Resolution...\")\n        test_instance.test_dependency_resolution_simple(mock_pipelines)\n        print(\"‚úì Dependency resolution tests passed\")\n        \n        print(\"\\n4. Testing Sequential Execution...\")\n        test_instance.test_sequential_execution(sample_data, mock_pipelines)\n        print(\"‚úì Sequential execution tests passed\")\n        \n        print(\"\\n5. Testing Parallel Execution...\")\n        test_instance.test_parallel_execution(sample_data, mock_pipelines)\n        print(\"‚úì Parallel execution tests passed\")\n        \n        print(\"\\n6. Testing Metadata Enrichment...\")\n        test_instance.test_composition_metadata_enrichment(sample_data, mock_pipelines)\n        print(\"‚úì Metadata enrichment tests passed\")\n        \n        print(\"\\n7. Testing Error Recovery...\")\n        test_instance.test_error_recovery_modes(sample_data, mock_pipelines)\n        print(\"‚úì Error recovery tests passed\")\n        \n        print(\"\\n8. Testing Composition Summary...\")\n        test_instance.test_composition_summary(sample_data, mock_pipelines)\n        print(\"‚úì Composition summary tests passed\")\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"üéâ ALL PIPELINECOMPOSER TESTS PASSED SUCCESSFULLY! üéâ\")\n        print(\"=\"*60)\n        print(\"\\nPipelineComposer Implementation Validation Complete:\")\n        print(\"‚úì Multi-stage workflow orchestration\")\n        print(\"‚úì Sequential and parallel execution strategies\")\n        print(\"‚úì Intelligent dependency resolution\")\n        print(\"‚úì Rich composition metadata for tool chaining\")\n        print(\"‚úì Comprehensive error recovery mechanisms\")\n        print(\"‚úì Performance monitoring and memory tracking\")\n        print(\"‚úì Integration with DataSciencePipeline framework\")\n        print(\"\\nPipelineComposer is ready for production use in LocalData MCP v2.0!\")\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        exit(1)\n"